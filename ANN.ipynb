{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b88b65c-e8ae-42a6-b646-8039f28130a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Program Files/Python312/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(20)\n",
    "random.seed(20)\n",
    "tf.random.set_seed(20)\n",
    "\n",
    "# Read and preprocess training data\n",
    "train_data_path = 'train_data.csv'  # Update with the correct path\n",
    "train_df = pd.read_csv(train_data_path, delimiter=';', header=None, skiprows=1, names=['ORB', 'SSIM', 'VGG16', 'GRADED'])\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(train_df[['ORB', 'SSIM', 'VGG16']])\n",
    "y_train = train_df['GRADED'].values\n",
    "\n",
    "# Define and compile the model\n",
    "model = Sequential([\n",
    "    Dense(10, input_dim=3, activation='relu'),  # Input layer with 3 inputs, hidden layer with 24 neurons\n",
    "    Dense(10, activation='relu'),  # Second hidden layer\n",
    "    Dense(1, activation='linear')  # Output layer with a single output (grade)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Use early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with fewer epochs and a larger validation split\n",
    "model.fit(X_train, y_train, epochs=200, validation_split=0.3, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Load and preprocess the test data\n",
    "test_data_path = 'tekk.csv'  # Update this path with the correct test data location\n",
    "test_df = pd.read_csv(test_data_path, delimiter=';', header=None, skiprows=1, names=['ORB', 'SSIM', 'VGG16', 'GRADED'])\n",
    "\n",
    "# Use the same scaler to normalize test features\n",
    "X_test = scaler.transform(test_df[['ORB', 'SSIM', 'VGG16']])\n",
    "y_test = test_df['GRADED'].values\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, mae = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {loss}, Test MAE: {mae}\")\n",
    "\n",
    "# Predict on the test data\n",
    "predicted_grades = model.predict(X_test, verbose=1)\n",
    "test_df['PREDICTED_GRADE'] = predicted_grades\n",
    "\n",
    "# Save predictions to a new CSV\n",
    "test_df.to_csv('test_data_with_predictions3.csv', index=False)\n",
    "\n",
    "print(f\"Predictions saved to 'test_data_with_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8a2105-5cfe-42cd-8f2e-e1d9ba04f430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
